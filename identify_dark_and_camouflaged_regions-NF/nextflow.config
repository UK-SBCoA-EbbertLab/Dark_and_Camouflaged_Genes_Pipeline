// CONFIGURATION FILE

process {

	/*
     * NOTE: 'lsamtools' is samtools compiled against 'libdeflate', which we
     * have found to be much faster than standard samtools that is compiled
     * against 'zlib'.
	 *
	 * Post: https://bioinformatics.stackexchange.com/questions/18538/samtools-sort-most-efficient-memory-and-thread-settings-for-many-samples-on-a-c
	 */
    withLabel: "lsamtools_collate_and_fastq_proc" {
        executor='slurm'
		queue = 'normal'
		cpus = 4
		memory = 8.GB
        clusterOptions='--time 30:00:00 --account coa_mteb223_uksr'
    }

    withLabel: "lsamtools_fastq_proc" {
        executor='slurm'
		queue = 'normal'
		cpus = 4
		memory = 10.GB
        clusterOptions='--time 20:00:00 --account coa_mteb223_uksr'
    }


    withLabel: "split_fastq_proc" {
        executor='slurm'
		queue = 'normal'
		cpus = 16
		memory = 2.GB
        clusterOptions='--time 40:00:00 --account coa_mteb223_uksr'
    }

	/*
	 * Heng Li seems to recommend ~5.5GB of mem per thread. Since we're splitting
	 * .fastqs into mini alignments, We're requesting 4 CPUs and 4 * 6GB = 24GB of mem.
	 * We'll round up to 25. This may not be the optimal combination. Needs to be tested.
	 */
    withLabel: "bwa_mem_proc" {
        executor='slurm'
		queue = 'normal'
		cpus = 4
		memory = 25.GB
        clusterOptions='--time 15:00:00 --account coa_mteb223_uksr'
    }

    withLabel: "minimap2_proc" {
        executor='slurm'
		queue = 'normal'
		cpus = 4
		memory = 25.GB
        clusterOptions='--time 50:00:00 --account coa_mteb223_uksr'
    }


	/*
	 * We found that the optimal CPU and mem settings for sambamba (for a single sample)
	 * are ~9 CPUs with ~9G per CPU. It actually continues to get better with even more
	 * CPUs, but not a ton. There's about a 6-minute difference between 6 CPUs @5G per
	 * thread (~28 min.) and 9 CPUs @5G per thread (~22 min.), but we can get more
	 * samples on with 6 CPUs. For this application, we're also splitting into mini
	 * .fastqs and .bams, so we shouldn't need nearly so many resources, anyway.
	 *
	 * We give a 20% buffer on memory because sambamba has a hard time staying within
	 * it's limits. So, total mem is 6 * 5 * 1.2 * 1024 = ~36864M
	 *
	 * Post: https://bioinformatics.stackexchange.com/questions/18538/samtools-sort-most-efficient-memory-and-thread-settings-for-many-samples-on-a-c
	 */
    withLabel: "csort_proc" {
        executor='slurm'
		queue = 'normal'
		cpus = 6
		memory = 36864.MB
        clusterOptions='--time 4:00:00 --account coa_mteb223_uksr'
    }


	/*
	 * We found that the optimal CPU and mem settings are ~6 CPUs with ~5G per CPU.
	 * We give a 30% buffer on memory because samtools has a hard time staying within
	 * it's limits. So, total mem is 6 * 5 * 1.3 * 1024 = ~47923M
	 */
    withLabel: "lsamtools_csort_proc" {
        executor='slurm'
		queue = 'normal'
		cpus = 6
		memory = 47923.MB
        clusterOptions='--time 4:00:00 --account coa_mteb223_uksr'
    }

    withLabel: "lsamtools_merge_proc" {
        executor='slurm'
		queue = 'normal'
		cpus = 6
		memory = 50.GB
        clusterOptions='--time 7:00:00 --account coa_mteb223_uksr'
    }

    withLabel: "RUN_DRF_PROC" {
        executor='slurm'
		queue = 'normal'
		cpus = 1
		memory = 100.GB
        clusterOptions='--time 50:00:00 --account coa_mteb223_uksr' 
    }

    withLabel: "COMBINE_DRF_OUTPUT_PROC" {
        executor='slurm'
		queue = 'normal'

		/*
		 * The combine_DRF.sh script spawns cpus / 2 background
		 * processes, and each of those will use 2 threads.
		 */
		cpus = 32

		/*
		 * In this case, define mem per CPU since SLURM allows that.
		 */
        clusterOptions='--time 5:00:00 --account coa_mteb223_uksr --mem-per-cpu=2G'
    }

    withLabel: "CREATE_BED_FILE_PROC" {
		executor='slurm'
		queue = 'normal'
		cpus = 10

		/*
		 * In this case, define mem per CPU since SLURM allows that.
		 */
		clusterOptions='--time 48:00:00 --account coa_mteb223_uksr --mem-per-cpu=5G'
    }

    withLabel: "MASK_GENOME_PROC" {
		executor='slurm'
		queue = 'normal'
		cpus = 1
		memory = 10.GB

		clusterOptions='--time 6:00:00 --account coa_mteb223_uksr'
    }


    withLabel: local {
        executor='local'
    }

}



// Define executor type and maximum queue size for jobs at once ##

executor {

    name="slurm"

	/*
	 * The max number of jobs Nextflow will submit at a given time. SLURM
	 * manages how many jobs can run at once, so not much reason to limit here.
	 *
	 * UKY MCC allows up to 10_000 job submissions, so we'll limit a bit below that.
	 */
    queueSize = 1_000
}

trace {
    enabled = true
    fields = 'task_id,hash,native_id,name,status,exit,submit,duration,realtime,%cpu,rss,%mem,peak_rss,peak_vmem,rchar,wchar'
}


// Point to singularity image with the tools necessary to run the pipeline
singularity {

    enabled = true
    process.container = "${projectDir}/../singularity/rescue_camo_variants.sif"
}

