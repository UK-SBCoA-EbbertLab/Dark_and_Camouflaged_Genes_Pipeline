// CONFIGURATION FILE

process {

    withLabel: "samtools_collate_and_fastq_proc" {
        executor='slurm'
		queue = 'normal'
		cpus = 4
		memory = 8.GB
        clusterOptions='--time 3:00:00 --account coa_mteb223_uksr'
    }

    withLabel: "split_fastq_proc" {
        executor='slurm'
		queue = 'normal'
		cpus = 1
		memory = 1.GB
        clusterOptions='--time 4:00:00 --account coa_mteb223_uksr'
    }

	/*
	 * Heng Li seems to recommend ~5.5GB of mem per thread. Since we're splitting
	 * .fastqs into mini alignments, We're requesting 4 CPUs and 4 * 6GB = 24GB of mem.
	 * This may not be the optimal combination. Needs to be tested.
	 */
    withLabel: "bwa_mem_proc" {
        executor='slurm'
		queue = 'normal'
		cpus = 4
		memory = 24.GB
        clusterOptions='--time 10:00:00 --account coa_mteb223_uksr'
    }

	/*
	 * We found that the optimal CPU and mem settings are ~7 CPUs with ~1G per CPU.
	 * We give a 30% buffer on memory because samtools has a hard time staying within
	 * it's limits. So, total mem is 7 * 1 * 1.3 * 1024 = ~9318M
	 */
    withLabel: "samtools_coordinate_sort_proc" {
        executor='slurm'
		queue = 'normal'
		cpus = 7
		memory = 9318.MB
        clusterOptions='--time 4:00:00 --account coa_mteb223_uksr'
    }

    withLabel: "samtools_merge_proc" {
        executor='slurm'
		queue = 'normal'
		cpus = 1
		memory = 50.GB
        clusterOptions='--time 7:00:00 --account coa_mteb223_uksr'
    }

    withLabel: "RUN_DRF_PROC" {
        executor='slurm'
		queue = 'normal'
		cpus = 1
		memory = 20.GB
        clusterOptions='--time 2:00:00 --account coa_mteb223_uksr' 
    }

    withLabel: "COMBINE_DRF_OUTPUT_PROC" {
        executor='slurm'
		queue = 'normal'
		cpus = 16

		/*
		 * In this case, define mem per CPU since SLURM allows that.
		 */
        clusterOptions='--time 8:00:00 --account coa_mteb223_uksr --mem-per-cpu=2G'
    }

    withLabel: "CREATE_BED_FILE_PROC" {
		executor='slurm'
		queue = 'normal'
		cpus = 16

		/*
		 * In this case, define mem per CPU since SLURM allows that.
		 */
		clusterOptions='--time 48:00:00 --account coa_mteb223_uksr --mem-per-cpu=6G'
    }

    withLabel: "MASK_GENOME_PROC" {
		executor='slurm'
		queue = 'normal'
		cpus = 1
		memory = 10.GB

		clusterOptions='--time 6:00:00 --account coa_mteb223_uksr'
    }


    withLabel: local {
        executor='local'
    }

}



// Define executor type and maximum queue size for jobs at once ##

executor {

    name="slurm"

	/*
	 * The max number of jobs Nextflow will submit at a given time. SLURM
	 * manages how many jobs can run at once, so not much reason to limit here.
	 *
	 * UKY MCC allows up to 10_000 job submissions, so we'll limit a bit below that.
	 */
    queueSize = 9_500
}

trace {
    enabled = true
    fields = 'task_id,hash,native_id,name,status,exit,submit,duration,realtime,%cpu,rss,%mem,peak_rss,peak_vmem,rchar,wchar'
}


// Point to singularity image with the tools necessary to run the pipeline
singularity {

    enabled = true
    process.container = "${projectDir}/../singularity/rescue_camo_variants.sif"
}

